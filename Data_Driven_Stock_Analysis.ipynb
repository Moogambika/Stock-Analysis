{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "-2SvPW8xsCTm",
        "outputId": "995989d4-e70a-42ff-ef5b-58ac0a5d86ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28848dce-4120-468c-a0e0-8f9ea0eacbe6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28848dce-4120-468c-a0e0-8f9ea0eacbe6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml --quiet\n",
        "import os, yaml, pandas as pd\n",
        "\n",
        "# 1. Upload your \"data\" folder again (contains monthly YAMLs)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload your ZIP of YAMLs here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\", \"r\") as z:\n",
        "    z.extractall(\"/content/data\")\n",
        "\n",
        "print(\"‚úÖ Extracted to /content/data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "797BAl0ftOPx",
        "outputId": "69fd5463-7a40-49f5-b3df-35aca79142d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted to /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, yaml, pandas as pd\n",
        "\n",
        "input_dir = \"/content/data/data\"      # ‚úÖ your dataset path\n",
        "output_dir = \"/content/csv_output\"    # folder for output CSVs\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "all_data = {}\n",
        "\n",
        "# Loop through all month folders and daily YAMLs\n",
        "for root, dirs, files in os.walk(input_dir):\n",
        "    for file in files:\n",
        "        if file.endswith((\".yaml\", \".yml\")):\n",
        "            file_path = os.path.join(root, file)\n",
        "            with open(file_path, \"r\") as f:\n",
        "                try:\n",
        "                    yaml_data = yaml.safe_load(f)\n",
        "                except Exception as e:\n",
        "                    print(\"‚ö†Ô∏è Error reading\", file_path, \":\", e)\n",
        "                    continue\n",
        "\n",
        "            # ‚úÖ Here top-level is a list\n",
        "            if isinstance(yaml_data, list):\n",
        "                for record in yaml_data:\n",
        "                    if isinstance(record, dict) and \"Ticker\" in record:\n",
        "                        symbol = record[\"Ticker\"]\n",
        "                        if symbol not in all_data:\n",
        "                            all_data[symbol] = []\n",
        "                        all_data[symbol].append(record)\n",
        "\n",
        "# Save one CSV per stock symbol\n",
        "for symbol, records in all_data.items():\n",
        "    df = pd.DataFrame(records)\n",
        "    output_path = os.path.join(output_dir, f\"{symbol}.csv\")\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"‚úÖ Saved {output_path} ({len(df)} rows)\")\n",
        "\n",
        "print(\"\\nüéâ Conversion complete!\")\n",
        "print(\"Total CSVs created:\", len(os.listdir(output_dir)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6a7wAfptR8Y",
        "outputId": "c5ead8ba-2d30-44d9-ecc3-6dc5b4ef1881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved /content/csv_output/SBIN.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BAJFINANCE.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TITAN.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ITC.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TCS.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/LT.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TATACONSUM.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/RELIANCE.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HCLTECH.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/JSWSTEEL.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ULTRACEMCO.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/POWERGRID.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/INFY.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TRENT.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BHARTIARTL.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TATAMOTORS.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/WIPRO.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TECHM.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/NTPC.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HINDUNILVR.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/APOLLOHOSP.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/M&M.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/GRASIM.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ICICIBANK.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ADANIENT.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ADANIPORTS.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BEL.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BAJAJFINSV.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/EICHERMOT.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/COALINDIA.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/MARUTI.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/INDUSINDBK.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ASIANPAINT.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/TATASTEEL.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HDFCLIFE.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/DRREDDY.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/SUNPHARMA.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/KOTAKBANK.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/SHRIRAMFIN.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/NESTLEIND.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/ONGC.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/CIPLA.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BPCL.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BRITANNIA.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/SBILIFE.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HINDALCO.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HEROMOTOCO.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/AXISBANK.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/HDFCBANK.csv (284 rows)\n",
            "‚úÖ Saved /content/csv_output/BAJAJ-AUTO.csv (284 rows)\n",
            "\n",
            "üéâ Conversion complete!\n",
            "Total CSVs created: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "input_dir = \"/content/csv_output\"   # folder with 50 CSVs\n",
        "all_dfs = []\n",
        "\n",
        "# loop through CSV files\n",
        "for root, dirs, files in os.walk(input_dir):   # include subfolders if any\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # ‚úÖ Do NOT add 'symbol' column; use 'Ticker' as is\n",
        "            all_dfs.append(df)\n",
        "\n",
        "# concat all into one DataFrame\n",
        "final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# save master file\n",
        "final_df.to_csv(\"/content/all_stocks.csv\", index=False)\n",
        "\n",
        "print(\"üéâ Done! Master CSV saved as /content/all_stocks.csv with\", len(final_df), \"rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY88q1udt_Sp",
        "outputId": "b8e94bc4-4dbf-40cd-c8b9-01da14eb2a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Done! Master CSV saved as /content/all_stocks.csv with 14200 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mapping: Ticker -> (Company Name, Sector)\n",
        "sector_map_dict = {\n",
        "    \"ADANIGREEN\": (\"ADANI ENTERPRISES\", \"MISCELLANEOUS\"),\n",
        "    \"ADANIPORTS\": (\"ADANI PORTS & SEZ\", \"MISCELLANEOUS\"),\n",
        "    \"APOLLOHOSP\": (\"APOLLO HOSPITALS\", \"MISCELLANEOUS\"),\n",
        "    \"ASIANPAINT\": (\"ASIAN PAINTS\", \"PAINTS\"),\n",
        "    \"AXISBANK\": (\"AXIS BANK\", \"BANKING\"),\n",
        "    \"BAJAJ-AUTO\": (\"BAJAJ AUTO\", \"AUTOMOBILES\"),\n",
        "    \"BAJFINANCE\": (\"BAJAJ FINANCE\", \"FINANCE\"),\n",
        "    \"BAJAJFINSV\": (\"BAJAJ FINSERV\", \"FINANCE\"),\n",
        "    \"BEL\": (\"BHARAT ELECTRONICS\", \"DEFENCE\"),\n",
        "    \"AIRTEL\": (\"BHARTI AIRTEL\", \"TELECOM\"),\n",
        "    \"BPCL\": (\"BPCL\", \"ENERGY\"),\n",
        "    \"CIPLA\": (\"CIPLA\", \"PHARMACEUTICALS\"),\n",
        "    \"COALINDIA\": (\"COAL INDIA\", \"MINING\"),\n",
        "    \"DRREDDY\": (\"DR. REDDYS LAB\", \"PHARMACEUTICALS\"),\n",
        "    \"EICHERMOT\": (\"EICHER MOTORS\", \"AUTOMOBILES\"),\n",
        "    \"GRASIM\": (\"GRASIM\", \"TEXTILES\"),\n",
        "    \"HCLTECH\": (\"HCL TECHNOLOGIES\", \"SOFTWARE\"),\n",
        "    \"HDFCBANK\": (\"HDFC BANK\", \"BANKING\"),\n",
        "    \"HDFCLIFE\": (\"HDFC LIFE INSURANCE\", \"INSURANCE\"),\n",
        "    \"HEROMOTOCO\": (\"HERO MOTOCORP\", \"AUTOMOBILES\"),\n",
        "    \"HINDALCO\": (\"HINDALCO\", \"ALUMINIUM\"),\n",
        "    \"HINDUNILVR\": (\"HINDUSTAN UNILEVER\", \"FMCG\"),\n",
        "    \"ICICIBANK\": (\"ICICI BANK\", \"BANKING\"),\n",
        "    \"INDUSINDBK\": (\"INDUSIND BANK\", \"BANKING\"),\n",
        "    \"INFY\": (\"INFOSYS\", \"SOFTWARE\"),\n",
        "    \"IOC\": (\"IOC\", \"ENERGY\"),\n",
        "    \"ITC\": (\"ITC\", \"FOOD & TOBACCO\"),\n",
        "    \"JSWSTEEL\": (\"JSW STEEL\", \"STEEL\"),\n",
        "    \"KOTAKBANK\": (\"KOTAK MAHINDRA BANK\", \"BANKING\"),\n",
        "    \"LT\": (\"L&T\", \"ENGINEERING\"),\n",
        "    \"M&M\": (\"M&M\", \"AUTOMOBILES\"),\n",
        "    \"MARUTI\": (\"MARUTI SUZUKI\", \"AUTOMOBILES\"),\n",
        "    \"NESTLEIND\": (\"NESTLE\", \"FOOD & TOBACCO\"),\n",
        "    \"NTPC\": (\"NTPC\", \"POWER\"),\n",
        "    \"ONGC\": (\"ONGC\", \"ENERGY\"),\n",
        "    \"POWERGRID\": (\"POWER GRID\", \"POWER\"),\n",
        "    \"RELIANCE\": (\"RELIANCE IND.\", \"ENERGY\"),\n",
        "    \"SBIN\": (\"SBI\", \"BANKING\"),\n",
        "    \"SBILIFE\": (\"SBI LIFE INSURANCE\", \"INSURANCE\"),\n",
        "    \"SHRIRAMFIN\": (\"SHRIRAM FINANCE\", \"FINANCE\"),\n",
        "    \"SUNPHARMA\": (\"SUN PHARMA\", \"PHARMACEUTICALS\"),\n",
        "    \"TATACONSUM\": (\"TATA CONSUMER\", \"FMCG\"),\n",
        "    \"TATAMOTORS\": (\"TATA MOTORS\", \"AUTOMOBILES\"),\n",
        "    \"TATASTEEL\": (\"TATA STEEL\", \"STEEL\"),\n",
        "    \"TCS\": (\"TCS\", \"SOFTWARE\"),\n",
        "    \"TECHM\": (\"TECH MAHINDRA\", \"SOFTWARE\"),\n",
        "    \"TITAN\": (\"TITAN\", \"RETAILING\"),\n",
        "    \"TRENT\": (\"TRENT\", \"RETAILING\"),\n",
        "    \"ULTRACEMCO\": (\"ULTRATECH CEMENT\", \"CEMENT\"),\n",
        "    \"WIPRO\": (\"WIPRO\", \"SOFTWARE\")\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "sector_df = pd.DataFrame(\n",
        "    [(company, sector, ticker) for ticker, (company, sector) in sector_map_dict.items()],\n",
        "    columns=[\"COMPANY\", \"sector\", \"Ticker\"]\n",
        ")\n",
        "\n",
        "# Save CSV\n",
        "sector_df.to_csv(\"sector_mapping.csv\", index=False)\n",
        "print(\"‚úÖ sector_mapping.csv with company names created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKRO2-gUE2zZ",
        "outputId": "2a5ec41f-f6da-47e8-e080-5c7efff7e20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ sector_mapping.csv with company names created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_analysis_csvs.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "INPUT = \"all_stocks.csv\"          # Master CSV with columns: Ticker, close, date, high, low, open, volume\n",
        "SECTOR_MAP = \"sector_mapping.csv\" # CSV with columns: COMPANY, sector, Ticker\n",
        "OUT_DIR = \"analysis_csvs\"         # Folder to save analysis CSVs\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Load CSV and parse date\n",
        "# ---------------------------\n",
        "df = pd.read_csv(INPUT, parse_dates=[\"date\"])\n",
        "df = df.sort_values([\"Ticker\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "sector_df = pd.read_csv(SECTOR_MAP)  # Contains COMPANY, sector, Ticker\n",
        "\n",
        "# Merge sector info into stock data\n",
        "df = df.merge(sector_df[[\"Ticker\", \"sector\"]], on=\"Ticker\", how=\"left\")\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Volatility Analysis\n",
        "# ---------------------------\n",
        "df[\"daily_return\"] = df.groupby(\"Ticker\")[\"close\"].pct_change()\n",
        "\n",
        "vol_df = df.groupby(\"Ticker\").agg(\n",
        "    std_dev_daily_return=(\"daily_return\", \"std\"),\n",
        "    avg_daily_return=(\"daily_return\", \"mean\"),\n",
        "    count_obs=(\"daily_return\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "vol_df.to_csv(os.path.join(OUT_DIR, \"volatility_analysis.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Cumulative Return Over Time\n",
        "# ---------------------------\n",
        "df[\"cumulative_return\"] = df.groupby(\"Ticker\")[\"daily_return\"].transform(lambda x: (1 + x).cumprod() - 1)\n",
        "\n",
        "cum_out = df.dropna(subset=[\"daily_return\"])[[\"date\", \"Ticker\", \"daily_return\", \"cumulative_return\"]]\n",
        "cum_out.to_csv(os.path.join(OUT_DIR, \"cumulative_return.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Sector-wise Performance\n",
        "# ---------------------------\n",
        "first_last = df.groupby(\"Ticker\").agg(\n",
        "    first_close=(\"close\", \"first\"),\n",
        "    last_close=(\"close\", \"last\"),\n",
        "    sector=(\"sector\", \"first\")\n",
        ").reset_index()\n",
        "\n",
        "first_last[\"yearly_return\"] = (first_last[\"last_close\"] - first_last[\"first_close\"]) / first_last[\"first_close\"]\n",
        "\n",
        "sector_out = first_last[[\"Ticker\", \"sector\", \"yearly_return\"]]\n",
        "sector_out.to_csv(os.path.join(OUT_DIR, \"sector_performance.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Stock Price Correlation\n",
        "# ---------------------------\n",
        "close_pivot = df.pivot(index=\"date\", columns=\"Ticker\", values=\"close\")\n",
        "close_pivot = close_pivot.loc[:, close_pivot.count() > 10]  # drop extremely sparse tickers\n",
        "\n",
        "corr = close_pivot.pct_change().corr()\n",
        "\n",
        "# Flatten correlation matrix safely\n",
        "corr_flat = corr.reset_index().melt(id_vars=\"Ticker\", var_name=\"Ticker_2\", value_name=\"Correlation\")\n",
        "corr_flat.rename(columns={\"Ticker\": \"Ticker_1\"}, inplace=True)\n",
        "\n",
        "# Save correlation CSVs\n",
        "corr_flat.to_csv(os.path.join(OUT_DIR, \"stock_correlation.csv\"), index=False)\n",
        "corr.to_csv(os.path.join(OUT_DIR, \"stock_correlation_matrix.csv\"))\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Top 5 Gainers and Losers (Month-wise)\n",
        "# ---------------------------\n",
        "df[\"YearMonth\"] = df[\"date\"].dt.to_period(\"M\")\n",
        "monthly = df.groupby([\"Ticker\", \"YearMonth\"]).agg(\n",
        "    start_close=(\"close\", \"first\"),\n",
        "    end_close=(\"close\", \"last\")\n",
        ").reset_index()\n",
        "\n",
        "monthly[\"monthly_return\"] = (monthly[\"end_close\"] - monthly[\"start_close\"]) / monthly[\"start_close\"]\n",
        "monthly[\"YearMonth_str\"] = monthly[\"YearMonth\"].astype(str)\n",
        "\n",
        "out_rows = []\n",
        "for ym, grp in monthly.groupby(\"YearMonth_str\"):\n",
        "    grp_sorted = grp.sort_values(\"monthly_return\", ascending=False).copy().reset_index(drop=True)\n",
        "\n",
        "    # Top 5 gainers\n",
        "    top5 = grp_sorted.head(5).copy()\n",
        "    top5 = top5.assign(Rank=np.arange(1, len(top5)+1), Type=\"Gainer\")\n",
        "\n",
        "    # Bottom 5 losers\n",
        "    bottom5 = grp_sorted.tail(5).copy().sort_values(\"monthly_return\").reset_index(drop=True)\n",
        "    bottom5 = bottom5.assign(Rank=np.arange(1, len(bottom5)+1), Type=\"Loser\")\n",
        "\n",
        "    out_rows.append(pd.concat([top5, bottom5], ignore_index=True))\n",
        "\n",
        "top5_df = pd.concat(out_rows, ignore_index=True)\n",
        "top5_df = top5_df[[\"YearMonth_str\", \"Ticker\", \"monthly_return\", \"Rank\", \"Type\"]]\n",
        "top5_df.columns = [\"Month\", \"Ticker\", \"Monthly_Return\", \"Rank\", \"Type\"]\n",
        "top5_df.to_csv(os.path.join(OUT_DIR, \"top5_gainers_losers.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Done\n",
        "# ---------------------------\n",
        "print(\"‚úÖ Generated all analysis CSVs in folder:\", OUT_DIR)\n",
        "print(\"- volatility_analysis.csv\")\n",
        "print(\"- cumulative_return.csv\")\n",
        "print(\"- sector_performance.csv\")\n",
        "print(\"- stock_correlation.csv\")\n",
        "print(\"- stock_correlation_matrix.csv\")\n",
        "print(\"- top5_gainers_losers.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKTpOj7tAlqd",
        "outputId": "e28e1ba2-095b-4c31-e9ae-d507388bcb98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated all analysis CSVs in folder: analysis_csvs\n",
            "- volatility_analysis.csv\n",
            "- cumulative_return.csv\n",
            "- sector_performance.csv\n",
            "- stock_correlation.csv\n",
            "- stock_correlation_matrix.csv\n",
            "- top5_gainers_losers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZKMMclngtdy",
        "outputId": "f6da4200-669e-489a-9c11-c9d2b4743b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading mysql_connector_python-9.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sqlalchemy pymysql\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUkk1RfQg03y",
        "outputId": "3308df2c-b268-4b7a-bb1e-6117e3cc1985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: pymysql in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Bpr_vbk1N8",
        "outputId": "2abe2295-4beb-43aa-93d6-33fa8cbb87b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db_url = \"mysql+pymysql://sJS9A4ifsRxCyhp.root:cwllEsca2qrfcuJ1@gateway01.ap-southeast-1.prod.aws.tidbcloud.com:4000/stock_analysis\"\n",
        "engine = create_engine(db_url)"
      ],
      "metadata": {
        "id": "gezAkL5Zeb-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Step 2: Connect to TiDB Cloud\n",
        "conn = mysql.connector.connect(\n",
        "    host=\"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
        "    port=4000,\n",
        "    user=\"sJS9A4ifsRxCyhp.root\",\n",
        "    password=\"cwllEsca2qrfcuJ1\",\n",
        "    database=\"stock_analysis\",\n",
        ")\n",
        "\n",
        "# Step 3: Test connection\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT NOW()\")\n",
        "print(\"‚úÖ Connected! Current time:\", cursor.fetchone())\n",
        "\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kir2c16sgd56",
        "outputId": "d2cc54e6-fc06-4c16-e1ba-3ccb642e6a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Connected! Current time: (datetime.datetime(2025, 9, 10, 7, 41, 43),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to TiDB Cloud (mysql.connector auto-handles SSL)\n",
        "conn = mysql.connector.connect(\n",
        "    host=\"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
        "    port=4000,\n",
        "    user=\"sJS9A4ifsRxCyhp.root\",\n",
        "    password=\"cwllEsca2qrfcuJ1\",\n",
        "    database=\"stock_analysis\",\n",
        "    ssl_verify_identity=True\n",
        ")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Function to upload a DataFrame manually\n",
        "def upload_dataframe(df, table_name):\n",
        "    # Create table automatically based on DataFrame columns\n",
        "    cols = \", \".join([f\"`{c}` TEXT\" for c in df.columns])  # use TEXT for simplicity\n",
        "    cursor.execute(f\"DROP TABLE IF EXISTS `{table_name}`\")\n",
        "    cursor.execute(f\"CREATE TABLE `{table_name}` ({cols})\")\n",
        "\n",
        "    # Insert data\n",
        "    placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
        "    insert_sql = f\"INSERT INTO `{table_name}` VALUES ({placeholders})\"\n",
        "    cursor.executemany(insert_sql, df.astype(str).values.tolist())\n",
        "    conn.commit()\n",
        "    print(f\"‚úÖ Uploaded {table_name} ({len(df)} rows)\")\n",
        "\n",
        "# Upload all CSVs\n",
        "base_path = \"/content/analysis_csvs/\"\n",
        "files = {\n",
        "    \"volatility_analysis\": base_path + \"volatility_analysis.csv\",\n",
        "    \"cumulative_return\": base_path + \"cumulative_return.csv\",\n",
        "    \"sector_performance\": base_path + \"sector_performance.csv\",\n",
        "    \"stock_correlation\": base_path + \"stock_correlation.csv\",\n",
        "    \"stock_correlation_matrix\": base_path + \"stock_correlation_matrix.csv\",\n",
        "    \"top5_gainers_losers\": base_path + \"top5_gainers_losers.csv\"\n",
        "}\n",
        "\n",
        "for table, path in files.items():\n",
        "    df = pd.read_csv(path)\n",
        "    upload_dataframe(df, table)\n",
        "\n",
        "# Verify\n",
        "cursor.execute(\"SHOW TABLES\")\n",
        "print(\"Tables in TiDB:\", cursor.fetchall())\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_AUdAgFieoH",
        "outputId": "9f1cbe69-e231-4e28-e246-1d55fdc1b65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Uploaded volatility_analysis (50 rows)\n",
            "‚úÖ Uploaded cumulative_return (14150 rows)\n",
            "‚úÖ Uploaded sector_performance (50 rows)\n",
            "‚úÖ Uploaded stock_correlation (2500 rows)\n",
            "‚úÖ Uploaded stock_correlation_matrix (50 rows)\n",
            "‚úÖ Uploaded top5_gainers_losers (140 rows)\n",
            "Tables in TiDB: [('cumulative_return',), ('sector_performance',), ('stock_correlation',), ('stock_correlation_matrix',), ('top5_gainers_losers',), ('volatility_analysis',)]\n"
          ]
        }
      ]
    }
  ]
}